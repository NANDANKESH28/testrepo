{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "code",
            "source": "\ndf_data_3 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('SampleCSVFile_11kb.csv', 'project2-donotdelete-pr-spcjnkzf69knzk'))\ndf_data_3.take(5)\n# @hidden_cell\n# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n# You might want to remove those credentials before you share your notebook.\ncredentials_1 = {\n    'IAM_SERVICE_ID': 'iam-ServiceId-1d9ace79-76f0-4369-837a-8bc0e559e5f7',\n    'IBM_API_KEY_ID': 'm9Q5ZQoWCL_6z5lQ58c9M3PWzOj33D6nYfNedfpOFYI1',\n    'ENDPOINT': 'https://s3.eu-geo.objectstorage.service.networklayer.com',\n    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n    'BUCKET': 'project2-donotdelete-pr-spcjnkzf69knzk',\n    'FILE': 'SampleCSVFile_11kb.csv'\n}\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\n\nif os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n    endpoint_0be3380b83314b35b90020ed443b4354 = 'https://s3.eu-geo.objectstorage.softlayer.net'\nelse:\n    endpoint_0be3380b83314b35b90020ed443b4354 = 'https://s3.eu-geo.objectstorage.service.networklayer.com'\n\nclient_0be3380b83314b35b90020ed443b4354 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='m9Q5ZQoWCL_6z5lQ58c9M3PWzOj33D6nYfNedfpOFYI1',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url=endpoint_0be3380b83314b35b90020ed443b4354)\n\nbody = client_0be3380b83314b35b90020ed443b4354.get_object(Bucket='project2-donotdelete-pr-spcjnkzf69knzk',Key='SampleCSVFile_11kb.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_2 = pd.read_csv(body)\ndf_data_2.head()\nimport ibmos2spark, os\n# @hidden_cell\n\nif os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n    endpoint_0be3380b83314b35b90020ed443b4354 = 'https://s3.eu-geo.objectstorage.softlayer.net'\nelse:\n    endpoint_0be3380b83314b35b90020ed443b4354 = 'https://s3.eu-geo.objectstorage.service.networklayer.com'\n\ncredentials = {\n    'endpoint': endpoint_0be3380b83314b35b90020ed443b4354,\n    'service_id': 'iam-ServiceId-1d9ace79-76f0-4369-837a-8bc0e559e5f7',\n    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n    'api_key': 'm9Q5ZQoWCL_6z5lQ58c9M3PWzOj33D6nYfNedfpOFYI1'\n}\n\nconfiguration_name = 'os_0be3380b83314b35b90020ed443b4354_configs'\ncos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf_data_1 = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .load(cos.url('SampleCSVFile_11kb.csv', 'project2-donotdelete-pr-spcjnkzf69knzk'))\ndf_data_1.take(5)",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python37",
            "display_name": "Python 3.7 with Spark",
            "language": "python3"
        },
        "language_info": {
            "mimetype": "text/x-python",
            "nbconvert_exporter": "python",
            "name": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.10",
            "file_extension": ".py",
            "codemirror_mode": {
                "version": 3,
                "name": "ipython"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}